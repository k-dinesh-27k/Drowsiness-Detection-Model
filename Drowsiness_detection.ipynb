{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe==0.10.20 opencv-python-headless\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UODt4jbncztS",
        "outputId": "f2bfd4a0-96a1-48c8-caf9-de6149ca73d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe==0.10.20 in /usr/local/lib/python3.12/dist-packages (0.10.20)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.12/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (25.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (25.12.19)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (0.7.1)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (0.7.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (0.5.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from mediapipe==0.10.20) (0.2.1)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe==0.10.20) (2.0.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.20) (0.5.4)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.20) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe==0.10.20) (1.16.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (26.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe==0.10.20) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.20) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi->sounddevice>=0.4.4->mediapipe==0.10.20) (3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORT REQUIRED LIBRARIES\n",
        "import cv2 # computer vision for image reading ,Processing and Transforming and to calculate EAR\n",
        "import mediapipe as mp #Mediapipe for facial frecognition for facial points to compute EAR(eye aspect ratio)\n",
        "import numpy as np # Numpy for image tensor computations\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n"
      ],
      "metadata": {
        "id": "d-bUKwm3B3-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/latest/face_landmarker.task\n",
        "#importing pretrained facial points detector (facelandmarker) - Transfer lear"
      ],
      "metadata": {
        "id": "IBbmN6sjd60X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_options = python.BaseOptions(model_asset_path=\"face_landmarker.task\")\n",
        "\n",
        "options = vision.FaceLandmarkerOptions(\n",
        "    base_options=base_options, #Loading the imported model\n",
        "    running_mode=vision.RunningMode.VIDEO,\n",
        "    num_faces=1 # Limit on how many faces to detec\n",
        ")\n",
        "\n",
        "detector = vision.FaceLandmarker.create_from_options(options)\n"
      ],
      "metadata": {
        "id": "PEbI6Aq6d8gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EAR = (distance between upper and lower eyelids / width of eye lids)\n",
        "def compute_EAR(landmarks, eye_indices):\n",
        "    p = [landmarks[i] for i in eye_indices]\n",
        "\n",
        "    def dist(a, b):\n",
        "        return np.linalg.norm(np.array([a.x, a.y]) - np.array([b.x, b.y]))\n",
        "\n",
        "    vertical1 = dist(p[1], p[5])\n",
        "    vertical2 = dist(p[2], p[4])\n",
        "    horizontal = dist(p[0], p[3])\n",
        "\n",
        "    return (vertical1 + vertical2) / (2.0 * horizontal)\n"
      ],
      "metadata": {
        "id": "KZFG8Wnbd-ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Video input and output paths\n",
        "input_video = \"/content/input.mp4\"\n",
        "output_video = \"output.mp4\"\n",
        "#Reading video using cv2\n",
        "cap = cv2.VideoCapture(input_video)\n",
        "\n",
        "fps = cap.get(cv2.CAP_PROP_FPS) #calculates Fps of an video\n",
        "w  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))#Width of frame of the video\n",
        "h  = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))#height of frame of the video\n",
        "\n",
        "out = cv2.VideoWriter(output_video,\n",
        "                      cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "                      fps, (w, h))\n",
        "\n",
        "LEFT_EYE  = [33,160,158,133,153,144] #Landmarks of left eye\n",
        "RIGHT_EYE = [362,385,387,263,373,380]#Landmarks of right eye\n",
        "\n",
        "frame_timestamp = 0\n",
        "drowsy_counter = 0 #Counter to store the count of number of frames the person closed his eyes\n",
        "EAR_THRESHOLD = 0.23 #Threshold for checking if the eye is closed or not , below 0.23 is closed\n",
        "FRAMES_THRESHOLD = 50 #Number of frames to check if person is drowsy or not\n"
      ],
      "metadata": {
        "id": "e7xOIXyHeEkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while cap.isOpened(): #iterate through frames\n",
        "    ret, frame = cap.read() #ret returns True if frame exist , frame reads the numpy tensor of image in BGR format\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)#Converting BGR to RGB\n",
        "\n",
        "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb) #Transform the numpy image tensor into the required format of mediapipe\n",
        "\n",
        "    result = detector.detect_for_video(mp_image, frame_timestamp) # this detects the face and returns 436 landmarks of face\n",
        "    frame_timestamp += int(1000 / fps)\n",
        "\n",
        "    status = \"Awake\"\n",
        "\n",
        "    if result.face_landmarks:\n",
        "        landmarks = result.face_landmarks[0]\n",
        "\n",
        "        left_EAR  = compute_EAR(landmarks, LEFT_EYE) #Computing EAR for left eye\n",
        "        right_EAR = compute_EAR(landmarks, RIGHT_EYE)#computing EAR for right eye\n",
        "\n",
        "        EAR = (left_EAR + right_EAR) / 2.0 # Average of both to see if he closed both are not\n",
        "\n",
        "        if EAR < EAR_THRESHOLD: #If less EAR less than threshold , drowsy counter increases Else again set to '0'\n",
        "            drowsy_counter += 1\n",
        "        else:\n",
        "            drowsy_counter = 0\n",
        "\n",
        "        if drowsy_counter > FRAMES_THRESHOLD:#If eyes were closed more than 50 frames then set status to 'DROWSY'\n",
        "            status = \"DROWSY!\"\n",
        "\n",
        "        cv2.putText(frame, f\"EAR:{EAR:.3f}\", (30,40),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,0), 2) #Overlay the message of the score of EAR\n",
        "\n",
        "    color = (0,255,0) if status==\"Awake\" else (0,0,255) #Overlay the message if the person is awake or not\n",
        "\n",
        "    cv2.putText(frame, status, (30,80),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3) #Add the overlays to the current frame\n",
        "\n",
        "    out.write(frame) #Write each frame to the video\n",
        "\n",
        "cap.release()\n",
        "out.release()\n"
      ],
      "metadata": {
        "id": "Jh8KG_vTeHEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"output.mp4\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Dyfl0oE0eIK4",
        "outputId": "d7ba20df-3977-464b-f283-9419cd34b6bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b9003b11-8831-48df-bc0b-0dbb825805e5\", \"output.mp4\", 10540234)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}